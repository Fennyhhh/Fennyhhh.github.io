---
layout:     post
title:      "论文阅读——MSDNet： Multi-scale Dense Networks For Resource Efficient Image Classification"
subtitle:   " \"图像分类\""
date:       2019-02-10 15:42:00
author:     "Fenny"
header-img: "img/post-bg-2015.jpg"
tags:
    - 论文阅读
---

# 文章标题
MSDNet： Multi-scale Dense Networks For Resource Efficient Image Classification

# 文章来源
2018ICML

# 文章思路
当前最新、性能最好的模型要把它应用在真实环境上会有一定的困难。因为这些模型往往比较大，需要更多的计算资源，而真实环境中计算资源和时间等都有很大的限制，所以难以运用上这些模型。这篇文章针对资源限制这一问题，提出了解决方案。

# 实现方法
* 对于每个测试样本（一张图片），存在有限的计算资源预算B>0，且每个样本的B都不同。相当于每次给模型输入一张图像，然后根据计算资源预算来给出预测结果。对于一个个样本集Dtest={x1,...,xM}，存在已知的有限的计算资源B>0，则：
（1）计算量小于B/M属于easy样本，花费更少的计算量；
（2）计算量大于B/M属于difficult样本，花费更大的计算量。
对于第二种情况，这里就引进了一种思想：对于简单图像仅采用浅层的速度较快的网络来分类，而对于难分类图像再采用深层的速度慢一点的网络来分类。作者在网络中间引入了多个分类出口(intermediate classifiers)，在卷积神经网络的每一层上面添加一个分类器，将当前层的feature map 作为图像的representation，输入到分类器中进行分类，那么当在某个时间点，需要输出模型的结果时，就可以把最近的分类的结果作为输出。而且在条件2下，也可以将easy图像的结果尽早的输出。
* 但是直接简单的添加分类的做法，对于分类器的分类效果是有影响的 ，这主要有两个方面的原因：
（1）浅层次的分类不能获取图像的高层语义特征
（2）浅层次的分类器对于后面图形分类的精度的影响，早期的分类器可能导致网络早期层次的优化更利于早期图形分类器达到更好的效果，而不利于后面的分类的优化。
* 为了解决这个问题，作者引入了两个解决办法：Multi-Scale Feature Maps与Dense Connectivity。 
## Multi-Scale Feature Maps： 
MSDNet采用了多个尺度来获取图形的抽象特征，分为两个部分的串联。1.上层同尺度的特征的卷积。2.上层上个尺度feature map的降采样（diagonal connection）。这样可以对同层的分类器得到更好的分类结果，产生了高水平的特征表达，即使少量几层网络也适合用来分类，同时当使用全网络来评估时也保持着高精度。
## Dense Connectivity：
在已存在于网络的每一层的特征表达添加额外的features，特征重用提高特征利用率，同时隐含着深度监督，反向传播的时候，每个分类都可以通过shortcut 对某一层的产生直接的影响，让权重向对每个分类效果更好的方向更新。

# 网络结构
！[msdnet](/msdnet.jpg)
一个scale为3的4层MSDNet网络的结构如上图所示, 水平方向的结构与DenseNet相似，文中称水平的操作是对前层相同scale的feature maps进行concat，结果为regular conv；垂直方向与对角线的操作是进行down-sampling，得到的结果是strided conv.网络的第一层很特殊，通过降采样来获取不同尺度的特征。
### 分类器与损失函数：
有在最后的分类器前，还有几层结构：两个通道维为128的3×3的降采样conv+2×2 avgpooling+linear layers损失函数是由各个分类器损失函数的加权求和，论文每个分类器的权值都为1；分类器并没有连接着某一层的所有scale的特征，而是选择使用scale最大的feature maps来做分类，即coarsest scale的feature maps。
### 减少计算量:
论文采用了两个方法，一个是network reduction。随着网络的深度的不断增加，神经网络可以得到图像的高层次的语义特征。所以对于尺度的变化没有什么必要了，所以就采用每多少层，就减少一个尺度。另外对于一个分类器，这个分类器的特征是来自于对角的特征的，所以进行分类的时候，就先计算这些特征。


# 总结与思考
我们通常看到的进行特征融合的方法，是把深层的强语义特征经过上采样，与下采样具有更强位置和更细节特征的特征图进行融合。而MSDNet针对浅层弱语义的问题，直接把中间层的特征，沿着宽度的方向再进行降采样得到具有比较强语义的feature maps之后再用来分类，解决了如何利用浅层特征来解决easy样本的问题。但前者是对融合后的每一层特征进行预测，因而每一层的特征都具有较强的能力，而后者根据计算资源B以及一个阈值θk来决定使用哪层的特征进行预测，更倾向于解决如何利用浅层特征来解决easy样本的问题。这篇文章是从计算资源有限的角度出发，从简单和复杂样本这个角度去解决问题，而我们是从样本本身的问题出发而有了这个想法，但从方法上来说，其实是一样的，并且这篇文章考虑了很多细节的问题，是非常值得借鉴的。
