---
layout:     post
title:      "论文阅读——STC: A Simple to Complex Framework forWeakly-supervised Semantic Segmentation"
subtitle:   " \"弱监督语义分割\""
date:       2019-09-27 12:56:00
author:     "Fenny"
header-img: "img/post-bg-2015.jpg"
tags:
    - 论文阅读
---

# 文章标题
STC: A Simple to Complex Framework forWeakly-supervised Semantic Segmentation
# 文章来源
2016，IEEE Transactions on Pattern Analysis & Machine Intelligence
# 动机
作者在此之前提出了一个proposal-based solution（基于候选区域的方法），该方法利用proposal构建一个object localization map（目标定位图）作为语义分割网络训练的监督信息。<br>
该方法先提取proposals，然后通过一个HCP（Hypothesed -CNN-Pooling）网络，判断出proposal的类别信息，筛选出具有高置信度的proposal。进而利用multi-instance learning的思想对这些proposal做re-ranking。我们把图片每一个proposal的特征和其他图片包含这个类别的proposal的特征做距离度量，获取最短距离，然后做累加，统计出哪些proposal与其他标有该类别的图片较近。我们利用排序后的proposal对图片的像素进行投票，从而获取了包含物体位置信息的localization map。最后利用该localization map作为监督信息来训练一个语义分割网络。*文章名：《Learning to segment with image-level annotations》*<br>
但是proposal-based方法存在很大的缺点。首先，产生proposal的过程本身就很难，从proposal中提取特征又非常耗时。另外，通过累加的方式做定位可能会引入大量的false positive（如背景像素），因此得到的localization map并不是很准。因此当时的性能并不是很好，在Pascal VOC上的语义分割性能在43%（mIoU）左右。<br>
所以作者想了另外一个方法，即通过salience map（显著图）来解决弱监督的语义分割问题。从网络上获取的图片大致可以分为两种情况，一种是simple images，背景干净，只含有单个类别的目标；另一种是complex images，背景嘈杂并含有多类目标。对于简单图片，我们可以通过显著图预测出其最显著的区域。在显著图中，像素的值越高区域越亮，就越可能是大家关注的物体，同时在已知其图像标签的情况下，我们可以建立起像素和图像标签的关联。<br>

# 研究方法
该方法本质上是对同一个结构的网络利用不同的监督信息训练了三次。<br>
1. Initial DCNN<br>
由DRFI方法生成Saliency Map，再结合Image level label，通过多标签交叉熵损失函数来训练分割网络来训练网络。<br>
利用简单图片的显著图作为监督信息。由于简单图片往往只包含了一类物体，基于显著图可以获取每一个像素有多大的概率属于前景物体或者背景。我们利用显著图作为监督信息并采用multi-label cross-entropy loss训练出一个initial DCNN（I-DCNN），使得该网络具备一定语义分割能力。<br>
2. Enhanced DCNN<br>
由于I-DCNN在训练过程中，使用DRFI会有很大噪声，这个DCNN就是对上一个DCNN的refine。<br>
利用I-DCNN对训练样本做预测，同时又根据image-level label剔除一些噪声，由此可得到针对简单图片的segmentation mask，进而基于预测出来的segmentation mask，按照全监督的卷积神经网络通用的损失函数去训练一个Enhanced DCNN（E-DCNN），进一步提升网络的语义分割能力。<br>
3. Powerful DCNN<br>
引入更多的复杂图片，结合E-DCNN和其image-level label，预测segmentation mask，进而用它们作为监督信息重新训练一个更好的Powerful DCNN(P-DCNN)<br>
![stc](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/stc.jpg)
4. 多标签交叉熵损失函数<br>
假设训练集中有C个类。用OI= {1,2，...，C}，OP= {0,1,2，...，C}分别表示图像级和像素级的类别集标签，其中0表示背景类。分割网络由f(•)过滤，其中所有的卷积层过滤给定的图像I。 f(•)产生一个h*w*(c+1)维的激活输出，其中h和w分别表示每个通道的特征图的高度以及宽度。引入了一个多标签交叉熵损失函数来训练基于显著图的分割网络，其中每个像素能够以不同的概率自适应地归结于前景类别和背景。<br>
需要指出的是，我们也可以利用SaliencyCut 生成基于生成的显著图的前景/背景分割掩模。然后，可以使用单标签交叉熵损失函数进行训练。我们将这个方案与我们提出的方法进行比较，发现VOC 2012的性能将下降3％。原因是一些显著性检测结果是不准确的。因此，直接应用SaliencyCut生成分割掩模将引入许多噪声，这对训练I-DCNN是有害的。然而，基于提出的多标签交叉熵损失，正确的语义标签仍然有助于优化性能，可以减少低质量显著图所带来的负面影响。<br>



# 总结
这篇文章是对之前弱监督常用方法——多实例学习的思想的一个突破。用显著性检测结果作为初始掩膜，用语义分割网络和简单到复杂图片的区分不断优化弱监督语义分割结果也是非常自然而然地想法。但这个工作的弊端主要是必须依赖大量的简单图片做训练。那么能不能直接由复杂图片训练出高性能的弱监督的模型呢？






