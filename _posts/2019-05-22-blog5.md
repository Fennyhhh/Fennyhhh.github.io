---
layout:     post
title:      "论文阅读——A Two-Stream Deep Fusion Framework for High-Resolution Aerial Scene Classification"
subtitle:   " \"图像分类,孪生网络,遥感\""
date:       2019-05-22 9:12:00
author:     "Fenny"
header-img: "img/post-bg-2015.jpg"
tags:
    - 论文阅读
---

# 文章标题
A Two-Stream Deep Fusion Framework for High-Resolution Aerial Scene Classification

# 文章来源
Computational Intelligence and Neuroscience2018
# 文章思路
作者认为精心设计的特征表示方法和分类器可以提高分类的准确性，于是文章针对高分辨率航空影像的场景分类任务，提出了一个双流的深度卷积神经网络架构，使用两个CNN作为特征提取器，其次，采用两种特征融合策略融合两种不同类型的深度卷积特征（RGB和显著性特征），最后，使用ELM分类器进行最终分类。该网络在四个数据集上进行了实验（UC Merced、WHU-RS、AID、NWPU-RESISC45），取得了良好的分类效果。
# 网络结构
![tsdff](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/tsdff.jpg)
网络流程包括4个过程：
1. 基于不可见显着性检测对航拍图像进行预处理。
2. 利用原始RGB流和显着流从两种航拍图像中提取特征。 这两个流使用深度卷积神经网络来提取特征。
3. 融合提取的两组特征。
4. 使用ELM分类器进行空中场景分类。
* 预处理主要是原始RGB图片的显著性检测（显著性检测方法来源于Z. Zheng, T. Zhang, and L. Yan, “Saliency model for object detection: searching for novel items in the scene,” Optics Expresss, vol. 37, no. 9, pp. 1580–1582, 2012.）。
* 网络由两支CNN组成，第一支被称为原始RGB流，这一支网络主要是采用原始的遥感影像捕捉场景目标的外在信息，第二支网络的输入则是显著性流，这一支网络采用由原始影像生成的显著性检测结果图作为输入，捕获图像中的显著信息。这两个分支的网络架构是相同的。（这一部分的CNN结构，作者进行了3个尝试，分别采用了caffeNet、VGG16、GoogleNet作为特征提取器）
然后，文章采用了两个特征融合的方法对经过两支CNN提取后的特征进行融合，分别为串联融合特征（concatenate）和并联融合特征（相加），这两种都是比较经典简单的特征融合方式。最后采用了ELM作为分类器。
# 总结与展望  
该文章采用了两个CNN作为特征提取器提取两个特征，一个是显著性特征，一个是由原始RGB提取的特征。显著性特征会提取出图片中的显著目标和物体，作为分类的另一个先验知识。但是对于场景更为复杂的图片来说，显著性检测方法只能检测出前景比较突出的物体，但当目标很小或者场景更依赖与背景信息的时候，这种显著性特征是否还会有效也还是一个值得商榷的问题。但我们可以参考该文章将两个分支结合起来分类的做法，并且可以尝试改换分类器。
