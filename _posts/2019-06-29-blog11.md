---
layout:     post
title:      "从图像分割到语义分割"
subtitle:   " \"语义分割，图像分割\""
date:       2019-06-29 14:26:00
author:     "Fenny"
header-img: "img/post-bg-2015.jpg"
tags:
    - 研究回顾
---

# 一、图像语义分割的概念及原理
图像语义分割是图像理解的基石，在自动驾驶系统（具体为街景识别与理解）、无人机应用（着陆点判断）以及穿戴式设备等应用中举足轻重。图像是由许多像素（Pixel）组成，而“语义分割”是将每一个像素按照图像中表达语义含义的不同进行分组（Grouping）／分割（Segmentation）。图像语义分割是指机器能够自动分割并识别出图像中的内容，如图1所示，给出一个人骑摩托车的照片，机器判断后应当能够生成右侧图，红色标注为人，绿色是车（黑色表示背景background）。<br> 
![语义分割示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/语义分割示意图.jpg)<br> 
图1 语义分割示意图<br> 
# 二、语义分割常用的算法
##	1. 深度学习时代前的语义分割
从最简单的像素级别“阈值法”（Thresholding methods）、基于像素聚类的分割方法（Clustering-based segmentation methods）到“图划分”的分割方法（Graph partitioning segmentation methods），在深度学习（Deep learning, DL）到来之前，图像语义分割方面的工作可谓“百花齐放”。在此，我们仅以“Normalized cut” [1]和“Grab cut” [2]这两个基于图划分的经典分割方法为例，介绍一下深度学习时代前语义分割方面的研究。<br> 
### 1.1 Normalized Cut[1]
在Deeplearning技术快速发展之前，就已经有了很多做图像分割的技术，其中比较经典的是一种被称为“Normalized cut”的图划分方法，简称“ N-cut ”。<br> 
Normalized cut （N-cut）方法[1]是基于图划分（Graph partitioning）的语义分割方法中最著名的方法之一，于 2000 年 Jianbo Shi 和 Jitendra Malik 发表于相关领域顶级期刊 TPAMI。通常，传统基于图划分的语义分割方法都是将图像抽象为图（Graph）的形式 G=（V，E） （V 为图节点，E 为图的边），然后借助图理论（Graph theory）中的理论和算法进行图像的语义分割。常用的方法为经典的最小割算法（Min-cut algorithm）。不过，在边的权重计算时，经典 min-cut 算法只考虑了局部信息。如图2所示，以二分图为例（将 G 分为不相交的 , 两部分），若只考虑局部信息，那么分离出一个点显然是一个 min-cut，因此图划分的结果便是类似n1或n2这样的离群点，而从全局来看，实际应分成的组却是左右两大部分。<br> 
![Min-cut algorithm示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/Min-cutalgorithm示意图.jpg)<br>
图2 Min-cut algorithm示意图[1]<br> 
针对这一情形，N-cut 则提出了一种考虑全局信息的方法来进行图划分（Graph partitioning），即，将两个分割部分 A,B , 与全图节点的连接权重（assoc(A,V) 和 assoc(B,V)）考虑进去：<br> 
如此一来，在离群点划分中的某一项会接近 1，而这样的图划分显然不能使得是一个较小的值，故达到考虑全局信息而摒弃划分离群点的目的。这样的操作类似于机器学习中特征的规范化（Normalization）操作，故称为Normalized cut。N-cut不仅可以处理二类语义分割，而且将二分图扩展为 K 路（k-way）图划分即可完成多语义的图像语义分割，如图3所示。<br> 
![N-cut分割示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/N-cut分割示意图.jpg)<br>
图3 N-cut分割示意图[1]<br> 
### 1.2 Grab cut[2]
Grab cut [2]是微软剑桥研究院于 2004 年提出的著名交互式图像语义分割方法。与 N-cut 一样，grab cut 同样也是基于图划分，不过 grab cut 是其改进版本，可以看作迭代式的语义分割算法。Grab cut 利用了图像中的纹理（颜色）信息和边界（反差）信息，只要少量的用户交互操作即可得到比较好的前后背景分割结果。<br> 
在 Grab cut 中，RGB 图像的前景和背景分别用一个高斯混合模型（Gaussian mixture model, GMM）来建模。两个 GMM 分别用以刻画某像素属于前景或背景的概率，每个 GMM 高斯部件（Gaussian component）个数一般设为 。接下来，利用吉布斯能量方程（Gibbs energy function）对整张图像进行全局刻画，而后迭代求取使得能量方程达到最优值的参数作为两个 GMM 的最优参数。GMM 确定后，某像素属于前景或背景的概率就随之确定下来。<br> 
在与用户交互的过程中，Grab cut 提供两种交互方式：一种以包围框（Bounding box）为辅助信息；另一种以涂写的线条（Scribbled line）作为辅助信息。以图4为例，用户在开始时提供一个包围框，grab cut 默认的认为框中像素中包含主要物体／前景，此后经过迭代图划分求解，即可返回扣出的前景结果，可以发现即使是对于背景稍微复杂一些的图像，grab cut 仍有不俗表现。<br> 
![Grabcut分割示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/Grabcut分割示意图.jpg)<br>
图4 Grab cut分割示意图[2]<br> 
不过，在处理下图时，grab cut 的分割效果则不理想。此时，需要额外人为的提供更强的辅助信息：用红色线条／点标明背景区域，同时用白色线条标明前景区域。在此基础上，再次运行 grab cut 算法求取最优解即可得到较为满意的语义分割结果<br> Grab cut 虽效果优良，但缺点也非常明显，一是仅能处理二类语义分割问题，二是需要人为干预而不能做到完全自动化。<br> 
![人为辅助后的Grabcut分割结果.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/人为辅助后的Grabcut分割结果.jpg)<br>
图5 人为辅助后的Grab cut分割结果[2]<br> 
其实在深度学习时代到来前，语义分割工作大多是根据图像像素自身的低阶视觉信息（Low-level visual cues）来进行图像分割，且需要人工干预，无法达到端对端的效果。虽然这样的方法没有算法训练阶段，往往计算复杂度不高，但是在较困难的分割任务上（如果不提供人为的辅助信息），其分割效果并不能令人满意。<br> 

##  2.深度学习时代的语义分割
在计算机视觉步入深度学习时代之后，语义分割同样也进入了全新的发展阶段，以全卷积神经网络（Fully convolutional networks，FCN）为代表的一系列基于卷积神经网络“训练”的语义分割方法相继提出，屡屡刷新图像语义分割精度。<br> 
### 2.1 FCN[3]
卷积神经网络（CNN）被成功运用于图像语义分割之前，卷积神经网络，如AlexNet、VGG、GoogleNet等网络模型，已经在图像分类、目标检测等领域上取得了非常理想的效果。比如在将CNN 用于图像分类时，网络输入的内容是图像，经过数次卷积和池化操作后输出特征图，再使用全连接层将二维特征图转换为一维向量，输出内容为一组固定长度的向量，是图像属于某一类的概率。<br> 
图6所示为AlexNet网络用于图像分类示意图，将图片输入网络中，经过卷积核池化操作提取特征后，将二维的特征图输入全连接层，最后获得一个长度为1000的一维输出向量,表示输入图像属于每一类的概率, 该图像属于“tabby cat”这一类的概率最高，因此分类结果为“tabby cat”。但此种网络结构中的全连接层破坏掉了图片的空间信息，并不能对图像进行像元级别的预测。而使用卷积操作提取图像特征可以保留图片的相对位置信息，因此全卷积神经网络应运而生。<br> 
![用于图像识别分类的卷积神经网络示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/用于图像识别分类的卷积神经网络示意图.jpg)<br>
图6 用于图像识别分类的卷积神经网络示意图<br>
全卷积网络（Fully Convolutional Networks，FCN）[3]是于2014年Berkeley的Long等人提出的，该方法在用于图像分类的经典网络的基础上进行了改进，将全连接层换为卷积层，经过反卷积操作，输出一张与输入图像尺寸一致的图片。<br>
FCN的反卷积操作是整个网络结构中最关键的操作，输入图像经过卷积池化后，尺寸较原始输入尺寸要小，反卷积层对最后一个卷积层输出的特征图进行上采样, 图像中的空间信息得以保留，输出的图像与输入图像尺寸相同，这样可对输入图像中的每个像元都产生预测,对图像中的每一个像元都进行分类。 <br>
整个网络的架构如图7所示。<br>
![FCN结构示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/FCN结构示意图.jpg)<br>
图7  FCN结构示意图[3]<br>
池化操作会降低卷积层所得特征图的维度，会损失图片中的大量精细信息，多次池化操作后输出的特征图中所含信息是低分辨率的，较为粗糙的深层信息。为了充分的利用图像中浅层精细信息（高分辨率）和深层的粗糙信息（低分辨率），FCN采用了跳跃结构（skip layer），如图8所示，将不同池化层的输出的特征图进行上采样后优化输出，所得效果更优。<br>
![FCN中跳跃结构示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/FCN中跳跃结构示意图.jpg)<br>
图8 FCN中跳跃结构（skip-layer）示意图[3]<br>
FCN得到的结果虽然不够精细，对图像中的细节不敏感，在提取对象边界时过于模糊与平滑，且跳跃结构需要经过FCN-32S、FCN-16S和FCN-8s的三次训练才能获得最优效果。虽然FCN不尽完善，但其全新的思路对语义分割领域有着重大影响，后续的SegNet、Dilated convolution等网络结构都借鉴了了FCN的思路，并在此基础上进行改进。<br>
### 2.2编码-解码结构的分割网络<br>
在语义分割网络中，编码器-解码器架构的网络发展得十分迅速。编码器能够逐步降低池化层的空间维度，解码器则一步步恢复目标细节和空间维度。通常从编码器到解码器之间，会存在一些快捷的连接（shortcut connections），使得解码器能够更好地恢复目标细节。以下是几个常用的编码-解码结构的网络。<br>
#### 2.2.1 SegNet[4]
SegNet[4]（如图9所示）采用编码-解码的方法构造了一个深度全卷积神经网络用于逐像素的分割。其编码网络提取图像特征，图像尺寸不断缩小，解码网络将编码后的尺寸缩小的特征图不断上采样放大到原来的尺寸。<br>
![SegNet结构示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/SegNet结构示意图.jpg)<br>
图9  SegNet结构示意图[4]<br>
SegNet改进了FCN网络中对图像上采样的方法，在上采样过程中引入池化索引功能，将编码网络每一个池化层的池化索引保存,并且传递到后面对称的上采样层。<br>
![SegNet上采样示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/SegNet上采样示意图.jpg)<br>
图10  SegNet上采样示意图<br>
如图10所示，SegNet的上采样过程是将编码后输出特征图的值6、8、3和4, 通过之前保存的最大池化索引映射到新的特征图的相应位置，其他的位置由于值缺失，则用0填充。Segnet保留池化操作时的位置信息，上采样时直接将数据放在原先的位置，FCN则是采用反卷积和双线性插值，每一个像素都是运算后的结果。且FCN网络中为了提升预测精度而引入跳跃连接，因此在训练网络时需要分多次逐步训练以达到最优精度。而SegNet仅在解码过程中运用最大池化索引（max-pooling indices），便可以提升模型描述边界的能力，减少端到端的训练参数，提升整个模型的运行效率，并且仅需做一些微调，这种方式就可以被任何的编码-解码结构网络所采用。<br>
SegNet网络在CamVid 道路场景数据集和SUN RGB-D室内场景数据集上进行训练，对比FCN网络，其结构更为优雅，效果更好，刻画边界的能力更强，且训练时所需内存和时间更少。<br>

#### 2.2.2 U-Net[5]
成功训练一个深度神经网络需要大量已标注的训练样本。而医疗图像数据集通常非常小，且传统的语义分割方法在面对比较难分割的情况（如相互接触到的同类物体）通常性能表现不佳。在这篇论文中，作者提出了一个新的网络U-Net[5]和训练策略。为了更有效的利用标注数据，作者使用数据扩张的方法(data augmentation)来增加训练数据，这样使网络能从非常小的训练样本中学习，更具有鲁棒性，当它在少于 40 张图的生物医学数据集上训练时，IOU 值仍能达到 92%。不仅如此，这个网络非常的快，对一个512*512的图像进行分割，使用GPU运行，只需要不到一秒的时间。与FCN对比，U-net没有使用VGG等CNN模型作为预训练模型，因为U-net做的是医学图像的二值分割，不需要用ImageNet的预训练模型，而且U-net结构是可以根据自己的数据集自由加深网络结构的，比如在处理具有更大的感受野的目标的时候；另一个区别是u-net在进行浅层特征融合的时候，采用的是叠加的做法，而不是FCN中的求和操作。<br>
![U-Net网络结构.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/U-Net网络结构.jpg)<br> 
图11 U-Net网络结构[5]<br>
图11展示了U-net网络结构，它由contracting path 和 expansive path组成。contracting path是典型的卷积网络架构。<br>它的架构是一种重复结构，每次重复中都有2个卷积层和一个pooling层，卷积层中卷积核大小均为3*3，激活函数使用ReLU，两个卷积层之后是一个2*2的步长为2的max pooling层。每一次下采样后都把特征通道的数量加倍。contracting path中的每一步都首先使用反卷积(up-convolution)，每次使用反卷积都将特征通道数量减半，特征图大小加倍。反卷积过后，将反卷积的结果与contracting path中对应步骤的特征图拼接起来。contracting path中的特征图尺寸稍大，将其修剪过后进行拼接。对拼接后的map进行2次3*3的卷积。最后一层的卷积核大小为1*1，将64通道的特征图转化为特定深度（分类数量，二分类为2）的结果。网络总共23层。<br>

 

### 2.3 Dilated Convolutions [6]
在分割网络中，采用了空洞卷积（Dilated convolution）的网络结构也不胜枚举。<br>
FCN 的一个不足之处在于，由于池化层的存在，响应张量的大小（长和宽）越来越小，但是FCN的设计初衷则需要和输入大小一致的输出，因此 FCN 做了上采样。但是上采样并不能将丢失的信息全部无损地找回来。<br>
对此，dilated convolution 是一种很好的解决方案——既然池化的下采样操作会带来信息损失，那么就把池化层去掉。但是池化层去掉随之带来的是网络各层的感受野（Receptive field）变小，这样会降低整个模型的预测精度。Dilated convolution 的主要贡献就是，如何在去掉池化下采样操作的同时，而不降低网络的感受野。<br>
以 3×3 的卷积核为例，传统卷积核在做卷积操作时，是将卷积核与输入张量中“连续”的 3×3 的 patch 逐点相乘再求和（如图12（a），红色圆点为卷积核对应的输入“像素”，绿色为其在原输入中的感知野）。而 dilated convolution 中的卷积核则是将输入张量的 3×3 patch 隔一定的像素进行卷积运算。如图12（b）所示，在去掉一层池化层后，需要在去掉的池化层后将传统卷积层换做一个“dilation=2”的 dilated convolution 层，此时卷积核将输入张量每隔一个“像素”的位置作为输入 patch 进行卷积计算，可以发现这时对应到原输入的感知野已经扩大（dilate）为 ；同理，如果再去掉一个池化层，就要将其之后的卷积层换成“dilation=4”的 dilated convolution 层，如图12（c）所示。这样一来，即使去掉池化层也能保证网络的感受野，从而确保图像语义分割的精度。<br>
![Dilatedconvolution示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/Dilatedconvolution示意图.jpg)<br> 
图12 Dilated convolution 示意图[6]<br>
# 优化技巧<br>
为了进一步优化语义分割网络的性能，使其关注到更多图像的细节，研究者通常会采用后端优化和多尺度整合两种思路进行网络结构的优化。以下是几种优化技巧的介绍。<br>
## 1.后端优化<br>
### 1.1条件随机场（Conditional random field，CRF）[7]<br>
来自 CNN 的原始标签一般都是“缺失（patchy）”图像，在图像中有一些小区域的标签可能不正确，因此无法匹配其周围的像素标签。为了解决这种不连续性，可以用一种平滑的形式。我们需要确保目标占据图片中的连续区域，这样给定的像素和其周围像素的标签就是一样的。<br>
为了解决这个问题，当下许多以深度学习为框架的图像语义分割工作都是用了条件随机场（Conditional random field，CRF）作为最后的后处理操作来对语义预测结果进行优化，即使用原始图像中像素的相似性重新精炼 CNN 的标签。<br>
CRF 将图像中每个像素点所属的类别都看作一个变量  ，然后考虑任意两个变量之间的关系，建立一个完全图（如下图所示）。<br> 
![CRF示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/CRF示意图.jpg)<br> 
图13 CRF示意图<br>
在全连接的 CRF 模型中，对应的能量函数为：<br>
一元项，表示Xi像素对应的语义类别, 其类别可以由 FCN  或者其他语义分割模型的预测结果得到；二元项可将像素之间的语义关系考虑进去。例如，和“鸟”这样的像素在物理空间是相邻的概率，应该要比“天空”和“鱼”这样像素的相邻概率大。最后通过对 CRF 能量函数的优化求解，得到对 FCN 的图像语义预测结果进行优化，得到最终的语义分割结果。值得一提的是，已经有工作[8]将原本与深度模型训练割裂开的 CRF 过程嵌入到神经网络内部，即，将 FCN+CRF 的过程整合到一个端到端的系统中，这样做的好处是 CRF 最后预测结果的能量函数可以直接用来指导 FCN 模型参数的训练，而取得更好的图像语义分割结果。<br>
### 1.2马尔科夫随机场(MRF)<br>
在Deep Parsing Network[9]中作者使用了MRF，它的公式具体的定义和CRF类似，只不过作者对二元势函数进行了修改。
作者加入λk为label context，因为只是定义了两个像素同时出现的频率，而λk可以对一些情况进行惩罚。 比如，人可能在桌子旁边，但是在桌子下面的可能性就更小一些。所以这个量可以学习不同情况出现的概率。 而原来的距离d(i,j)只定义了两个像素间的关系，作者在这儿加入了个triple penalty，即还引入了j附近的z，这样描述三方关系便于得到更充足的局部上下文。<br>
具体结构如图14所示：<br>
![MRF示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/MRF示意图.jpg)<br> 
图14 MRF示意图<br>
这个结构的优点在于它将平均场构造成了CNN，能联合训练并且可以one-pass inference，不用迭代。<br>
## 2 多尺度整合<br>
除了后端优化这个常用的技巧外，在语义分割中考虑多尺度特征的输入和融合也是提高性能的有效手段。有研究者将这两种优化手段结合起来，取得了令人满意的效果。<br>
### 2.1 PSPNet[10]<br>
针对FCN-based网络没有引入足够的上下文信息及不同感受野下的全局信息而存在分割出现错误的现象,如关系不匹配（Mismatched Relationship），类别易混淆及类别不显眼（Inconspicuous Classes）等情况，作者提出了一个名为pyramid scene parsing network (PSPNet) 的网络，网络架构如图15所示：
![PSPNet网络示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/PSPNet网络示意图.jpg)<br> 
图15 PSPNet网络示意图<br>
将图片输入网络，用卷积网络ResNet作为基础网络提取特征，得到特征图，特征图进行池化操作后，接1x1卷积，将不同尺度的特征上采样，然后把特征进行融合，作为金字塔池化的特征输出。金字塔池化包括四种不同的尺度： 1x1, 2x2, 3x3, 6x6，每个尺度下的filter个数为1/N（N为卷积网络最后输出的通道个数），将每个尺度的特征进行融合，进行了结构化预测。
### 2.2Attention to scale[11]<br>
多尺度融合是提高语义分割性能的一个重要手段。在语义分割网络中，主要有两种融合多尺度特征的方法，第一种skip-net,是取网络中多个中间层的特征并合并成一个特征；第二种share-net,是对输入图像进行尺度上的变换，得到不同尺度的输入图像，然后分别输入给网络，得到不同尺度的输入图像的特征，以形成多尺度的特征。
作者基于第二种share-net网络结构，向网络提供多个大小不一的输入图像，然后合并产生的特征进行像素级分类，在这篇文章中，作者还提出了一种注意机制，学习在每个像素点位置上融合多尺度特征，并与多尺度输入图像和注意模型一起进行训练。所提出的注意模型不仅优于平均池化和最大池化的效果，而且能够在不同的位置和尺度上判断可视化特征的重要性。网络结构如图16所示。<br>
![Attentiontoscale网络.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/Attentiontoscale网络.jpg)<br> 
图16 Attention to scale网络<br>
### 2.3带孔空间金字塔池化（Atrous Spatial Pyramid Pooling,ASPP）<br>
Deeplab-v1[12]采用了空洞卷积的结构和全连接条件随机场相结合的方式进行语义分割。DeepLab-v2 [13] 在Deeplab-v1[12]的基础上提出了带孔空间金字塔池化 (Atrous Spatial Pyramid Pooling, ASPP)，用不同空洞率的空洞卷积层并行来获得多尺度信息，以提高语义分割性能。Deeplab v2关注的是如何在Deeplab v1的基础上进行改进，更好地提取出图像特征。作者将提取特征的主干网络换为了ResNet,这个改进提高了性能；并且添加了图像空间金字塔池化，从而更好的提取了多尺度图像信息。<br>
作者使用了多个并行的、不同采样率的卷积核，以同时切割大的物体和小的物体。同样有两种使用方法，一个是先resize再过卷积核，另一个是直接过卷积。这些卷积核全是rate不同的空洞卷积，可以说是用空洞卷积来实现了感受野不一样的空间金字塔池化。<br>
![ASPP结构示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/ASPP结构示意图.jpg)<br> 
图17 ASPP结构示意图<br>
### 2.4 GCN 全局卷积网络（Global Convolutional Network ，GCN）[14]<br>
作者提出了拥有大面积Kernel的编码-解码结构。语义分割需要对对象进行分割和分类，由于全连接层是不可分割的结构，且虽然更深度的神经网络比如ResNet拥有更大的接受视野，但研究表明网络往往从一个很小的区域收集信息（有效接受域），所以作者用非常大的卷积核来代替。 较大的内核在计算上花费很大，并且参数更多。因此，K*K卷积核与1*K+K*1卷积核或者k*1+1*K卷积核近似。该模型在论文中被称作全局卷积网络（GCN），网络结构如图18所示。 在架构中，ResNet（没有任何dilated卷积）组成编码部分，而GCNs和反卷积构成解码部分，该网络还使用了一个简单的边界细化（BR）的残余块。<br>
![GCN网络示意图.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/GCN网络示意图.jpg)<br>
图18 GCN网络示意图<br>

以上介绍的这些网络结构的优化技巧是在语义分割中较常用的类型，这些网络的思路大体为基础网络结构（FCN-based,编码结构或加入了空洞卷积的FCN网络）提取特征，再融合图像的多尺度特征，最后进行后端优化，以此来不断提升语义分割网络的性能。<br>
# 三、展望<br>
基于深度学习的图像语义分割技术虽然可以取得相比传统方法突飞猛进的分割效果，但是其对数据标注的要求过高：不仅需要海量图像数据，同时这些图像还需提供精确到像素级别的标记信息（Semantic labels）。因此，越来越多的研究者开始将注意力转移到弱监督（Weakly-supervised）条件下的图像语义分割问题上。在这类问题中，图像仅需提供图像级别标注（如，有“人”，有“车”，无“电视”）而不需要昂贵的像素级别信息即可取得与现有方法可比的语义分割精度。在最近的研究中，
近期的另一个工作重点是使用对抗性训练获得更高阶的一致性。受生成对抗网络（GAN）的启发，Luc等人训练了用于语义分割的标准CNN以及试着学习区分真实图分割和预测图分割的对抗网络[15]。分割网络旨在对抗网络无法从真实分割中区别出来的预测分割。<br>
![ssuan.jpg](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/ssuan.jpg)<br>
图19 Semantic Segmentation using Adversarial Networks<br>
另外，实例级别（Instance level）的图像语义分割问题也同样热门。该类问题不仅需要对不同语义物体进行图像分割，同时还要求对同一语义的不同个体进行分割。在以往的研究中，深度神经网络往往是单任务的，比如图像分类(AlexNet, VGG16等等)，图像分割(以FCN为代表的一众论文)，目标检测(R-CNN，Fast R-CNN和Fatser R-CNN，以及后来的YOLO和SSD）。而在最新的研究中，上述的任务往往被集成了，通过一个框架完成，代表就是Mask R-CNN[16]。语义分割能对图像做出像素级别的预测，将包含不同语义信息的物体区分开来，而实例分割可以将图像中同一语义类的不同对象也进行分割。MaskR-CNN是在Faster R-CNN网络的基础上进行改进，并添加了Mask分支，将图像分类、目标识别和实例分割结合成一个网络结构。<br>

# 四、语义分割常用评价指标
图像分割中通常使用许多标准来衡量算法的精度。这些标准通常是像素精度和IOU的变种，以下是几种常用的逐像素标记的精度标准。为了便于解释，假设如下：共有k+1个类（从L0到Lk,其中包含一个空类（即背景），Pij表示真实类别为i但被预测为类j的像素数量。即Pii表示真正的数量，而Pij Pji则分别被解释为假正和假负。）
1.	Pixel Accuracy(PA，像素精度)：这是最简单的度量，为标记正确的像素占总像素的比例。 
2. Mean Pixel Accuracy(MPA，均像素精度)：是PA的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。 
3. Mean Intersection over Union(MIoU，均交并比)：为语义分割的标准度量。其计算两个集合的交集和并集之比，在语义分割的问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。这个比例可以变形为正真数（intersection）比上真正、假负、假正（并集）之和。在每个类上计算IoU，之后平均。 这样的评价指标可以判断目标的捕获程度（使预测标签与标注尽可能重合），也可以判断模型的精确程度（使并集尽可能重合）
4. Frequency Weighted Intersection over Union(FWIoU，频权交并比):为MIoU的一种提升，这种方法根据每个类出现的频率为其设置权重。 
在以上所有的度量标准中，MIoU由于其简洁、代表性强而成为最常用的度量标准，大多数研究人员都使用该标准报告其结果。
# 五、语义分割常用数据集
## （1）SYNTHIA 
http://synthia-dataset.net 
计算机合成的城市道路驾驶环境的像素级标注的数据集。
## （2）CamVid 
http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/
街道场景数据集。
## （3）A2D 
https://web.eecs.umich.edu/~jjcorso/r/a2d/index.html 
A2D数据集是针对各种视觉问题的一个新颖的大型测试平台:视频级单标签和多标签动作识别，实例级对象分割/协同分割，以及像素级动作语义分割等。A2D有3782个视频，每个有效的动作元组至少有99个实例，视频被标记为像素级的演员和采样帧的动作。
## （4）Cityscapes 
https://www.cityscapes-dataset.com/ 
Cityscapes是一个街道场景分割数据集，它包含了从50个不同城市收集的5000个街道场景的高质量像素级标注，一共使用19个语义标签(属于7个超类别:ground、construction、object、nature、sky、human和vehicle)进行评估。训练、验证和测试集分别包含2975,500和1525幅图像。
## （5）1.PASCAL VOC2012
http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
大型自然图像数据集，包括17125张图像和20个对象类。
## （6）MS COCO
http://cocodataset.org
COCO大型自然图像数据集， 训练、验证和测试集包含20多万张图片和80个对象类别。
## （7）ADE20K
https://groups.csail.mit.edu/vision/datasets/ADE20K/
http://sceneparsing.csail.mit.edu/
室内室外场景分割数据集。包括20000张训练数据,2000张验证数据和若干张测试数据(需要上传结果到服务器才能得到结果)，一共150类。
## （8）Automatic Portrait Segmentation for Image Stylization
http://xiaoyongshen.me/webpage_portrait/index.html
人体肖像分割数据集



# 参考文献
[1] Shi, Jianbo, Malik, Jitendra. Normalized cuts and image segmentation[J]. IEEE Trans.pattern Anal.mach.intell, 2000, 22(8):888-905.<br>
[2] Rother C, Kolmogorov V, Blake A. "GrabCut": interactive foreground extraction using iterated graph cuts[C]// ACM SIGGRAPH. ACM, 2004:309-314.<br>
[3] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]// Computer Vision and Pattern Recognition. IEEE, 2015:3431-3440.<br>
[4]Badrinarayanan V, Kendall A, Cipolla R. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation.[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, PP(99):1-1.<br>
[5]Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation[C]// International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2015:234-241.<br>
[6] Fisher Yu and Vladlen Koltun. Multi-scale Context Aggregation by Dilated Convolutions. International Conference on Representation Learning, 2016.<br>
[7]Lafferty J D, Mccallum A, Pereira F C N. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data[C]// Eighteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. 2001:282-289.<br>
[8] Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional Random Fields as Recurrent Neural Networks[C]// IEEE International Conference on Computer Vision. IEEE, 2016:1529-1537.<br>
[9] Liu Z, Li X, Luo P, et al. Semantic Image Segmentation via Deep Parsing Network[J]. 2015:1377-1385.
[10] Zhao H, Shi J, Qi X, et al. Pyramid Scene Parsing Network[J]. 2016:6230-6239. <br>
[11] Chen L C, Yang Y, Wang J, et al. Attention to Scale: Scale-Aware Semantic Image Segmentation[J]. 2015:3640-3649.<br>
[12] Chen L C, Papandreou G, Kokkinos I, et al. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2018, 40(4):834-848.<br>
[13] Chen L C, Papandreou G, Kokkinos I, et al. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2018, 40(4):834-848.<br>
[14]Peng C, Zhang X, Yu G, et al. Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network[J]. 2017.<br>
[15]Luc P, Couprie C, Chintala S, et al. Semantic Segmentation using Adversarial Networks[J]. 2016.<br>
Chen L C, Papandreou G, Kokkinos I, et al. Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs[J]. Computer Science, 2014(4):357-361.<br>
[16]He K, Gkioxari G, Dollár P, et al. Mask R-CNN[J]. 2017.<br>

