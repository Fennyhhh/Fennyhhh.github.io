---
layout:     post
title:      "论文阅读：HMANet: Hybrid Multiple Attention Network for Semantic Segmentation in Aerial Images"
subtitle:   " \"语义分割,注意力\""
date:       2020-02-20 19:56:00
author:     "Fenny"
header-img: "img/post-bg-2015.jpg"
tags:
    - 论文阅读
---

# 背景
超高分辨率航空图像的语义分割是遥感图像理解中最具挑战性的任务之一。目前，基于深度卷积神经网络(DCNNs)的特征表示方法较多。具体来说，基于注意力的方法可以有效地捕获长期依赖关系，并进一步重构特征图以获得更好的表示。然而，仅仅从空间和通道注意的角度出发，加上自注意机制的庞大计算复杂度，很难对每个像素对之间的有效语义依赖关系进行建模。<br>
因此，作者提出了一个新的基于注意的框架，称为混合多注意网络(HMANet)，从空间、渠道和类别的角度自适应地捕获全局相关性。具体来说，一个嵌入了类信道注意(CCA)模块的类增强注意(CAA)模块可用于计算基于类别的相关性和重新校准类级信息。另外，引入了一个简单的区域转移注意(RSA)模块，通过区域表示来减少特征冗余和提高自我注意机制的效率。<br>
# 方法
## overview
* Backbone:ImageNet数据集上经过预训练的ResNet-101<br>
删除了最后两个下采样操作，并在stage3和stage4中使用了膨胀卷积，这也被称为后者的多重网格(MG)策略，从而保留了更多的空间信息。输出特征图X为为输入图像的1/8，特征图X将被馈送到两个并行的注意力分支。<br>
* 3个注意力模块：类增强注意(CAA)模块，类别通道增强注意(CCA)模块和区域随机打乱注意(Region Shuffle Attention, RSA)模块<br>
CAA模块旨在提取特征图的基于类的相关性，而CCA模块则通过类通道加权来改善特征重建的过程，以获得更好的上下文表示。网络的下层分支是RSA模块，与计算远程依赖关系时的原始注意力块相比，它大大减少了计算量和内存占用<br>
## CAA 类增强注意模块
自注意力机制本质上是数学中的一种矩阵乘法运算，其中二维分别是输入特征图的通道数{C}和高度与宽度{H×W}的乘积。 通过合并硬件维度，可以获得大小为C×C的标准信道亲和度矩阵，从而生成信道注意图，例如DANet中的信道注意模块。 直观上，非本地操作的定义限制了这种通道注意模块（即查询，键和值操作）中通道的扩展。 然而，当通道C之一被对应于由地面真相监督的分割图的通道替代时，它导致类别信息，同时保留了查询，键和值转换功能。 <br>
所提出的类的增强注意力的直觉是从类别信息的角度捕获远程上下文信息，即，显式地对数据集中的每个类别与输入要素多维数据集的每个通道之间的关系进行建模。 <br>
![caa](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/caa.jpg)<br>
## CCA 类别通道增强注意模块
CNN的高级语义被认为嵌入在通道维中，其中每个深层特征通道图都可以视为与类相关的响应。因此，作者提出了一个类通道注意（CCA）模块，以利用类通道依赖性并生成具有丰富且自适应上下文信息的新类亲和度图，该类亲和度图有效地嵌入了带有一些参数的CAA模块中。 <br>
![cca](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/cca.jpg)<br>
## RSA 区域随机打乱注意模块
就空间逐点相关表示而言，基于注意力的神经网络主要旨在通过自注意力机制或其变体捕获远程上下文依赖关系，最终生成密集的亲和矩阵。 然而，提出的区域改组注意力的重点是在稀疏和有效的方式重组后收获区域方面的依赖及其对应对象。 我们通过图5中的简单示意图来说明我们的方法。<br>
**区域表示**： 我们通过置换操作将输入特征图划分为区域，将每个特征图馈送到自适应全局平均池层中，然后再获得区域表示。 然后，我们合并区域的逐点表示，以生成整个输入要素的稀疏表示。 因此，为方便起见，可以有效地替换原始输入功能上的自注意力，而对合并后的对应对象则保持相同的注意力。<br>
**随机注意表示**： 尽管对合并特征的自我关注可以凭经验从所有位置捕获远程信息，但是像素间的连接仍然是模棱两可的。 为了从区域的角度利用更明确的上下文相关性，我们分别应用洗牌注意来交替合并相应的子区域并计算其自注意表示，从而分别实现空间信息的补充表示。 进一步的实验表明，两个子区域的注意力加权表示的级联可以有效地增强上下文相关性，优于像素级非局部算子。<br>
![rsa](https://github.com/Fennyhhh/Fennyhhh.github.io/blob/master/paper_img/rsa.jpg)<br>

# 结论
针对像素级注意机制存在的特征冗余和计算复杂度高的问题，提出了一种简单有效的解决方案。<br>
由于不同类别在某些情况下(如阴影或重叠)具有相似性，网络很难学习到这些精确的特征。也就是说，无效冗余信息的传播对特征表示是有害的，同时也是对GPU内存的浪费。因此，区域注意机制用来发掘更广泛的相关性。<br>
